# FLEX: A Benchmark for Evaluating Robustness of Fairness in Large Language Models

This repository is the official implementation of the paper "FLEX: A Benchmark for Evaluating Robustness of Fairness in Large Language Models", NAACL-findings 2025.

## Installation

To install the `lm-eval` package from the github repository, run:

```bash
git clone https://github.com/dhaabb55/FLEX/
cd FLEX
pip install -e .
```

## Running the Full Evaluation

```bash
bash run_FLEX.sh
```

Our code is based on [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)

Our data is based on [On Second Thought, Let's Not Think Step by Step: Bias and Toxicity in Zero-Shot Reasoning](https://github.com/SALT-NLP/chain-of-thought-bias)

## Cite as

```

```

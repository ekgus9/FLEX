# FLEX: A Benchmark for Evaluating Robustness of Fairness in Large Language Models

This repository is the official implementation of the paper "FLEX: A Benchmark for Evaluating Robustness of Fairness in Large Language Models", NAACL 2025.

## Installation

To install the `lm-eval` package from the github repository, run:

```bash
git clone https://github.com/dhaabb55/FLEX/
cd FLEX
pip install -e .
```

## Running the Full Evaluation

```bash
bash run_FLEX.sh
```

Our code is based on [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)

## Cite as

```

```
